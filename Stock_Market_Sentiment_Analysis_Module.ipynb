{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c2e951b",
   "metadata": {
    "id": "6c2e951b",
    "outputId": "ca04022b-7942-449f-ca41-d76ff241098b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "# import sentiment_mod as s\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cython\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "from lemminflect import getLemma\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "import re\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score,recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,precision_score,recall_score,confusion_matrix\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from xgboost import XGBClassifier\n",
    "%autosave 60\n",
    "# Autosaving every 60 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "23f1d823",
   "metadata": {
    "id": "23f1d823",
    "outputId": "b65340fe-f95f-4a9d-f7ac-8e79169a79c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>Industry body CII said #discoms are likely to ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>#Gold prices slip below Rs 46,000 as #investor...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>Workers at Bajaj Auto have agreed to a 10% wag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>#Sharemarket LIVE: Sensex off day’s high, up 6...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>#Sensex, #Nifty climb off day's highs, still u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5791 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "0     Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1     user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2     user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                     MNTA Over 12.00            1\n",
       "4                                      OI  Over 21.37            1\n",
       "...                                                 ...        ...\n",
       "5786  Industry body CII said #discoms are likely to ...         -1\n",
       "5787  #Gold prices slip below Rs 46,000 as #investor...         -1\n",
       "5788  Workers at Bajaj Auto have agreed to a 10% wag...          1\n",
       "5789  #Sharemarket LIVE: Sensex off day’s high, up 6...          1\n",
       "5790  #Sensex, #Nifty climb off day's highs, still u...          1\n",
       "\n",
       "[5791 rows x 2 columns]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/stock_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6d2ca072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('./data/stock_tweet_data.csv',sep = ';')\n",
    "df_1.dropna(inplace=True)\n",
    "def create_label(text):\n",
    "    if(text=='positive'):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "df_1['sentiment'] = df_1['sentiment'].apply(lambda x:create_label(x))\n",
    "df_1 = df_1[df_1['sentiment']==-1]\n",
    "df_1.drop(['id','created_at'],axis=1,inplace=True)\n",
    "df_1.columns = ['Text','Sentiment']\n",
    "df = pd.concat([df_1,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a2947ca1",
   "metadata": {
    "id": "a2947ca1"
   },
   "outputs": [],
   "source": [
    "df['label'] = df['Sentiment'].apply(lambda x: 'pos' if (x==1) else 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8674779e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    3685\n",
       "neg    2878\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8b5831",
   "metadata": {
    "id": "4e8b5831"
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_lemma(text)\n",
    "    text = text.lower()\n",
    "    text = remove_pos(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = [word for word in text.split(' ') if word not in sw_nltk]\n",
    "    return \" \".join(words)\n",
    "    \n",
    "def remove_numbers(string):\n",
    "    return ''.join([i for i in string if not i.isdigit()])\n",
    "    \n",
    "    \n",
    "    \n",
    "def remove_pos(text):\n",
    "    result_str = ''\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    allowed_pos = ['ADJ','ADV','NOUN','VERB','ADP']\n",
    "    for token in doc:\n",
    "        if token.pos_ in allowed_pos:\n",
    "            result_str +=str(token) + ' '\n",
    "    return result_str\n",
    "\n",
    "def remove_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    lemma = []\n",
    "    result_str = ''\n",
    "    for token in doc:\n",
    "        if token.lemma_ not in lemma:\n",
    "            lemma.append(token.lemma_)\n",
    "            result_str +=str(token.lemma_)+' '\n",
    "    return result_str\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "    \n",
    "def remove_html(text):\n",
    "    clean = re.compile(r'<.*?>')\n",
    "    return re.sub(clean,'',text)\n",
    "\n",
    "\n",
    "def final_processing(text):\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "#     words = word_tokenize(text)\n",
    "\n",
    "#     string = ''\n",
    "#     for word in words:\n",
    "#         lemma = lemmatizer.lemmatize(str(word),'a')\n",
    "#         string+=lemma+' '\n",
    "\n",
    "#     unique = set()\n",
    "#     for word in string.split():\n",
    "#         unique.add(word)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d445897",
   "metadata": {
    "id": "7d445897"
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d666b7",
   "metadata": {
    "id": "80d666b7"
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: final_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e15a8d",
   "metadata": {
    "id": "05e15a8d"
   },
   "outputs": [],
   "source": [
    "df = df[df['Text']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c6c44c",
   "metadata": {
    "id": "48c6c44c"
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "words = df['Text'].apply(lambda x:word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9975eb5",
   "metadata": {
    "id": "f9975eb5"
   },
   "outputs": [],
   "source": [
    "for sent in words:\n",
    "    for word in sent:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b470a93",
   "metadata": {
    "id": "8b470a93",
    "outputId": "7cdb776d-ec27-4f35-df46-b20e03c079cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49145"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5bef6f",
   "metadata": {
    "id": "3a5bef6f"
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())\n",
    "word_features = word_features[:7000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "368f3186",
   "metadata": {
    "id": "368f3186"
   },
   "outputs": [],
   "source": [
    "document = []\n",
    "pos_doc = df[df['label']=='pos']['Text']\n",
    "neg_doc = df[df['label']=='neg']['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf7f0b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2102"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e619f82",
   "metadata": {
    "id": "5e619f82"
   },
   "outputs": [],
   "source": [
    "for doc in pos_doc:\n",
    "    document.append((doc,'pos'))\n",
    "for doc in neg_doc:\n",
    "    document.append((doc,'neg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d295694",
   "metadata": {
    "id": "3d295694"
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in words:\n",
    "        features[w] = (w in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c579b020",
   "metadata": {
    "id": "c579b020"
   },
   "outputs": [],
   "source": [
    "random.shuffle(document)\n",
    "random.shuffle(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efccc3b2",
   "metadata": {
    "id": "efccc3b2"
   },
   "outputs": [],
   "source": [
    "feature_set = [(find_features(txt),cat) for (txt,cat) in document]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab1e3873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5780"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "837980b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb3aecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = feature_set[:5200]\n",
    "testing_set = feature_set[5200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d993866",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "LogReg_classifier = SklearnClassifier(LogisticRegression())\n",
    "KNN_classifier = SklearnClassifier(KNeighborsClassifier())\n",
    "SVM_classifier = SklearnClassifier(SVC())\n",
    "SGD_classifier = SklearnClassifier(SGDClassifier())\n",
    "RF_classifier = SklearnClassifier(RandomForestClassifier())\n",
    "XG_Classifier = SklearnClassifier(XGBClassifier())\n",
    "DT_Classifier = SklearnClassifier(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcd6cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SklearnClassifier(MultinomialNB())> accuracy percent: 75.51724137931033\n",
      "<SklearnClassifier(BernoulliNB())> accuracy percent: 75.0\n",
      "<SklearnClassifier(LogisticRegression())> accuracy percent: 77.93103448275862\n",
      "<SklearnClassifier(SVC())> accuracy percent: 78.96551724137932\n",
      "<SklearnClassifier(SGDClassifier())> accuracy percent: 76.20689655172413\n",
      "<SklearnClassifier(RandomForestClassifier())> accuracy percent: 77.24137931034483\n",
      "<SklearnClassifier(XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...))> accuracy percent: 77.58620689655173\n",
      "<SklearnClassifier(DecisionTreeClassifier())> accuracy percent: 70.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.73      0.56      0.64       202\n",
      "         pos       0.79      0.89      0.84       378\n",
      "\n",
      "    accuracy                           0.78       580\n",
      "   macro avg       0.76      0.73      0.74       580\n",
      "weighted avg       0.77      0.78      0.77       580\n",
      "\n",
      "[[114  88]\n",
      " [ 42 336]]\n"
     ]
    }
   ],
   "source": [
    "estimators = {MNB_classifier:'Multinomial_NB_classifier',\n",
    "              BNB_classifier:'Bernoulli_NB_classifier',\n",
    "              LogReg_classifier:'Log_reg_classifier',\n",
    "              SVM_classifier:'Support_VM_classifier',\n",
    "              SGD_classifier:'Stochastic_GD_classifier',\n",
    "              RF_classifier:'RandomForestClassifier',\n",
    "             XG_Classifier:'XGBoost',\n",
    "             DT_Classifier:'DecisionTree'}\n",
    "\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    \n",
    "    \n",
    "    def __init__(self,*classifiers):\n",
    "#         self._classifiers = classifiers\n",
    "        for classifier in classifiers:\n",
    "            classifier.train(training_set) \n",
    "            print(\"%s accuracy percent:\"%classifier, (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "#             print('\\n')\n",
    "            self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for classifier in self._classifiers:\n",
    "            votes.append(classifier.classify(features))\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self,features):\n",
    "        votes = []\n",
    "        for classifier in self._classifiers:\n",
    "            votes.append(classifier.classify(features))\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "    \n",
    "    \n",
    "voted_classifier = VoteClassifier(*estimators.keys())\n",
    "true_labels = [label for (features,label) in testing_set]\n",
    "pred_labels = [voted_classifier.classify(feature) for (feature,label) in testing_set]\n",
    "print(classification_report(true_labels,pred_labels))\n",
    "cm = confusion_matrix(true_labels,pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b044d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reliance goes down 3000 points\n",
      "('pos', 1.0)\n"
     ]
    }
   ],
   "source": [
    "def sentiment(text):\n",
    "    features = find_features(text)\n",
    "    return (voted_classifier.classify(features),voted_classifier.confidence(features))\n",
    "\n",
    "txt = str(input())\n",
    "print(sentiment(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "4d72cbd2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [868], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [label \u001b[38;5;28;01mfor\u001b[39;00m (features,label) \u001b[38;5;129;01min\u001b[39;00m testing_set]\n\u001b[1;32m----> 2\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m [voted_classifier\u001b[38;5;241m.\u001b[39mclassify(feature) \u001b[38;5;28;01mfor\u001b[39;00m (feature,label) \u001b[38;5;129;01min\u001b[39;00m testing_set]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(true_labels,pred_labels))\n\u001b[0;32m      4\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(true_labels,pred_labels)\n",
      "Cell \u001b[1;32mIn [868], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [label \u001b[38;5;28;01mfor\u001b[39;00m (features,label) \u001b[38;5;129;01min\u001b[39;00m testing_set]\n\u001b[1;32m----> 2\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mvoted_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (feature,label) \u001b[38;5;129;01min\u001b[39;00m testing_set]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(true_labels,pred_labels))\n\u001b[0;32m      4\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(true_labels,pred_labels)\n",
      "Cell \u001b[1;32mIn [23], line 25\u001b[0m, in \u001b[0;36mVoteClassifier.classify\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     23\u001b[0m votes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classifiers:\n\u001b[1;32m---> 25\u001b[0m     votes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mode(votes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\classify\\api.py:56\u001b[0m, in \u001b[0;36mClassifierI.classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m:return: the most appropriate label for the given featureset.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m:rtype: label\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overridden(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassify_many):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatureset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\classify\\scikitlearn.py:82\u001b[0m, in \u001b[0;36mSklearnClassifier.classify_many\u001b[1;34m(self, featuresets)\u001b[0m\n\u001b[0;32m     80\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorizer\u001b[38;5;241m.\u001b[39mtransform(featuresets)\n\u001b[0;32m     81\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [classes[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:821\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 821\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    824\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:874\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    869\u001b[0m all_proba \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    870\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], j), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m    872\u001b[0m ]\n\u001b[0;32m    873\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m--> 874\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proba \u001b[38;5;129;01min\u001b[39;00m all_proba:\n\u001b[0;32m    880\u001b[0m     proba \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:651\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[1;34m(predict, X, out, lock)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \n\u001b[0;32m    648\u001b[0m \u001b[38;5;124;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;124;03m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:923\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    921\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    922\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[1;32m--> 923\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    926\u001b[0m     proba \u001b[38;5;241m=\u001b[39m proba[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_]\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:776\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:781\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:119\u001b[0m, in \u001b[0;36mspmatrix.get_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m     new_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(shape, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39masformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_matrix\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124;03m\"\"\"Get shape of a matrix.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "true_labels = [label for (features,label) in testing_set]\n",
    "pred_labels = [voted_classifier.classify(feature) for (feature,label) in testing_set]\n",
    "print(classification_report(true_labels,pred_labels))\n",
    "cm = confusion_matrix(true_labels,pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548b504",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "b5a8a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "# cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "id": "7ae83b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_lemma(text)\n",
    "    text = text.lower()\n",
    "    text = remove_pos(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = [word for word in text.split(' ') if word not in sw_nltk]\n",
    "    return \" \".join(words)\n",
    "    \n",
    "def remove_numbers(string):\n",
    "    return ''.join([i for i in string if not i.isdigit()])\n",
    "    \n",
    "\n",
    "def remove_pos(text):\n",
    "    result_str = ''\n",
    "    doc = nlp(text)\n",
    "    allowed_pos = ['ADJ','ADV','VERB','ADP','NOUN']\n",
    "    for token in doc:\n",
    "        if token.pos_ in allowed_pos:\n",
    "            result_str +=str(token) + ' '\n",
    "    return result_str\n",
    "\n",
    "def remove_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    lemma = []\n",
    "    result_str = ''\n",
    "    for token in doc:\n",
    "        if token.lemma_ not in lemma:\n",
    "            lemma.append(token.lemma_)\n",
    "            result_str +=str(token.lemma_)+' '\n",
    "    return result_str\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "    \n",
    "def remove_html(text):\n",
    "    clean = re.compile(r'<.*?>')\n",
    "    return re.sub(clean,'',text)\n",
    "\n",
    "\n",
    "def final_processing(text):\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "#     words = word_tokenize(text)\n",
    "\n",
    "#     string = ''\n",
    "#     for word in words:\n",
    "#         lemma = lemmatizer.lemmatize(str(word),'a')\n",
    "#         string+=lemma+' '\n",
    "\n",
    "#     unique = set()\n",
    "#     for word in string.split():\n",
    "#         unique.add(word)\n",
    "    return text\n",
    "def remove_blank(text):\n",
    "    words = word_tokenize(text)\n",
    "    if len(words) < 1:\n",
    "        return pd.NA\n",
    "    else:\n",
    "        return text\n",
    "df = pd.read_csv('./data/stock_data.csv')\n",
    "df_1 = pd.read_csv('./data/stock_tweet_data.csv',sep = ';')\n",
    "df_1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(text):\n",
    "    if(text=='positive'):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "df_1['sentiment'] = df_1['sentiment'].apply(lambda x:create_label(x))\n",
    "df_1 = df_1[df_1['sentiment']==-1]\n",
    "df_1.drop(['id','created_at'],axis=1,inplace=True)\n",
    "df_1.columns = ['Text','Sentiment']\n",
    "df = pd.concat([df_1,df])\n",
    "df = df.sample(frac=1)\n",
    "df['Text'] = df['Text'].apply(lambda x:cleaner(x))\n",
    "df['Text'] = df['Text'].apply(lambda x:final_processing(x))\n",
    "df['Text'] = df['Text'].apply(lambda x:remove_blank(x))\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['Text']!='']\n",
    "df['label'] = df['Sentiment'].apply(lambda x: 'pos' if (x==1) else 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r'\\bong\\b', 'long', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eecd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = []\n",
    "for sent in df['label']:\n",
    "    if sent=='pos':\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "# X = cv.fit_transform(X)\n",
    "X = tfidf.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d470ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_validate\n",
    "tree_method = 'gpu_hist'\n",
    "def RandomForest(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators',100,200)\n",
    "#     max_depth = trial.suggest_int('max_depth',1,10)\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    scores = cross_validate(clf,X,y,cv = 3,scoring=['roc_auc'])\n",
    "    return (scores['test_roc_auc'].mean())\n",
    "def XGB(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators',100,400)\n",
    "    clf = XGBClassifier(n_estimators=n_estimators,\n",
    "                            eval_metric='auc',\n",
    "                            objective='binary:logistic')\n",
    "    scores = cross_validate(clf,X,y,cv = 3,scoring=['roc_auc'])\n",
    "    return (scores['test_roc_auc'].mean())\n",
    "def SVM(trial):\n",
    "    C = trial.suggest_int('C',1,10)\n",
    "    clf = SVC(C = C,\n",
    "             kernel = 'rbf')\n",
    "    scores = cross_validate(clf,X,y,cv = 3,scoring = ['roc_auc'])\n",
    "    return (scores['test_roc_auc'].mean())\n",
    "# def DecisionTree(trial):\n",
    "#     max_depth = trial.suggest_int('max_depth',1,9)\n",
    "#     clf = DecisionTreeClassifier(max_depth = max_depth,criterion = 'entropy')\n",
    "#     scores = cross_validate(clf,X,y,cv = 3,scoring = ['roc_auc'])\n",
    "#     return (scores['test_roc_auc'].mean())\n",
    "# def NB(trial):\n",
    "#     alpha = trial.suggest_int('alpha',1,10)\n",
    "#     clf = BernoulliNB(alpha = alpha)\n",
    "#     scores = cross_validate(clf,X,y,cv = 3,scoring=['roc_auc'])\n",
    "#     return (scores['test_roc_auc'].mean())\n",
    "# def KNN(trial):\n",
    "#     n_neighbors = trial.suggest_int('n_neighbors',5,50)\n",
    "#     clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "#     scores = cross_validate(clf,X,y,cv = 3,scoring=['roc_auc'])\n",
    "#     return (scores['test_roc_auc'].mean())    \n",
    "\n",
    "def Logreg(trial):\n",
    "    C = trial.suggest_int('C',1,100)\n",
    "    clf = LogisticRegression(C=C,max_iter=1000000)\n",
    "    scores = cross_validate(clf,X,y,cv = 3,scoring=['roc_auc'])\n",
    "    return (scores['test_roc_auc'].mean())\n",
    "\n",
    "\n",
    "objectives_list = [SVM,XGB,Logreg,RandomForest]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scores = []\n",
    "def tune(objective,est_name):\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=10)\n",
    "        scores.append({'model':est_name,'best_params':study.best_params,'roc_auc':study.best_value})\n",
    "        return scores\n",
    "    \n",
    "ensemble = [SVC,XGBClassifier,LogisticRegression,RandomForestClassifier]\n",
    "for objective,estimator_name in zip(objectives_list,ensemble):\n",
    "    tune(objective,estimator_name)\n",
    "    \n",
    "    \n",
    "df_models = pd.DataFrame(scores)\n",
    "df_models.sort_values(by='roc_auc',ascending=False,ignore_index=True,inplace=True)\n",
    "\n",
    "params = df_models['best_params']\n",
    "\n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c44880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b92e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for model,params in zip(df_models['model'],df_models['best_params']):\n",
    "    models.append(model(**params))\n",
    "for model in models:\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    true_labels = y_test\n",
    "    pred_labels = model.predict(X_test)\n",
    "    print(model)\n",
    "    print('-------------------CLASSIFICATION REPORT--------------------')\n",
    "    print('accuracy = ',model.score(X_test,y_test))\n",
    "    print(classification_report(true_labels,pred_labels))\n",
    "    cm = confusion_matrix(true_labels,pred_labels)\n",
    "    print(cm)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators= []\n",
    "count = 1\n",
    "for model in models:\n",
    "    estimators.append((str(count),model))\n",
    "    count+=1\n",
    "# estimators\n",
    "\n",
    "model = VotingClassifier(estimators,voting='hard')\n",
    "\n",
    "model.fit(X_train,y_train)  \n",
    "true_labels = y_test\n",
    "pred_labels = model.predict(X_test)\n",
    "print('-------------------CLASSIFICATION REPORT FOR ENSEMBLE--------------------')\n",
    "print('accuracy = ',model.score(X_test,y_test))\n",
    "print(classification_report(true_labels,pred_labels))\n",
    "cm = confusion_matrix(true_labels,pred_labels)\n",
    "print(cm)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21924b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(tfidf,model)\n",
    "\n",
    "def sentiment_1(text):\n",
    "    text = cleaner(text)\n",
    "    text = final_processing(text)\n",
    "    text_list = [text]\n",
    "    sentiment = pipeline.predict(text_list)\n",
    "    if sentiment==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "txt = input()\n",
    "print(sentiment_1(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57964f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "pipeline_f = open('./Pickled_algos_tfidf/pipeline.pkl','wb')\n",
    "joblib.dump(pipeline,pipeline_f)\n",
    "pipeline_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf238888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_1(text):\n",
    "    text = cleaner(text)\n",
    "    text = final_processing(text)\n",
    "    text_list = [text]\n",
    "    sentiment = pipeline_1.predict(text_list)\n",
    "    if sentiment==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "txt = input()\n",
    "print(sentiment_1(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bdc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1)\n",
    "model.fit(X_train,y_train)  \n",
    "true_labels = y_test\n",
    "pred_labels = model.predict(X_test)\n",
    "print('-------------------CLASSIFICATION REPORT FOR %s--------------------'%str(model))\n",
    "print('accuracy = ',model.score(X_test,y_test))\n",
    "print(classification_report(true_labels,pred_labels))\n",
    "cm = confusion_matrix(true_labels,pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b34b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "basicwords = tfidf.get_feature_names_out()\n",
    "basiccoeffs = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : basicwords, \n",
    "                        'Coefficient' : basiccoeffs})\n",
    "coeffdf = coeffdf.sort_values(by='Coefficient',ascending=False)\n",
    "coeffdf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffdf[coeffdf['Coefficient']<0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffdf[coeffdf['Coefficient']>0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a558942",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_pos = ['ADJ','ADV','VERB','INTJ','NOUN','ADP']\n",
    "text = nlp('Adani Ports signs brilliant agreement to with SPMK to enhance Haldia Dock’s volume')\n",
    "for token in text:\n",
    "    if token.pos_ in allowed_pos:\n",
    "        print(str(token) + ':' + token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bc39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447fc08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
